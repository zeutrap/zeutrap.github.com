---
layout: post
title: "Flickr架构"
description: ""
category: "architecture"
tags: ["all-time-favorites系列", "一日一架构", "架构", "译文"]
---
{% include JB/setup %}

本文翻译自 HighScalability.com的[Flickr Architecture](http://highscalability.com/flickr-architecture)一文。

Flickr既是我最喜欢的鸟的名字又是互联网界领先的图片分享站点的名字。Flickr面临着神奇的挑战：他们必须能高性能地处理不断扩充的新内容，不断增加的新用户和源源不断的新功能。他们是怎么做到的？

##信息源
* [Flickr and PHP](http://www.niallkennedy.com/blog/uploads/flickr_php.pdf)(早期文档)  
* [Capacity Planning for LAMP](http://www.kitchensoap.com/talks/MySQLConf2007-Capacity.pdf)  
* Dathan Pattishall的[Federation at Flickr: Doing Billions of Queries a Day](http://mysqldba.blogspot.com/2008/04/mysql-uc-2007-presentation-file.html)  
* Cal Henderson的[Building Scalable Web Sites](http://highscalability.com/book-building-scalable-web-sites)  
* Tim OReilly的[Database War Stories #3: Flickr ](http://radar.oreilly.com/archives/2006/04/database_war_stories_3_flickr.html)  
* [Cal Henderson的演讲](http://www.iamcal.com/talks/)。这里面有大量有用的信息。  

##平台
* PHP  
* MySQL  
* Shards  
* 使用Memcached提供缓存层  
* Squid用来做html和图片的反向代理  
* Linux(RedHat)  
* 使用Smarty生成模板  
* Perl  
* PEAR用来做XML和Email的解析  
* ImageMagick用来做图像处理  
* Java提供节点服务  
* Apache  
* 用SystemImager做部署  
* 用Ganglia做分布式系统监控  
* Subcon将系统配置文件存储到subversion库中简化集群的部署  
* Cvsup用来做网络内部文件集合的分发和更新

##数字
* 每天超过40亿次查询  
* 总计约3500万图片存储在squid缓存中  
* 约200万图片存储在squid内存中  
* 约4.7亿图片  
* memcached的qps为3.8万次请求每秒(总共1200万对象)  
* 2PB原始数据(周日的时候约为1.5TB)  
* 每天增加图片超过40万张  

##架构
* 可以从这个[幻灯片](http://www.slideshare.net/techdude/scalable-web-architectures-common-patterns-and-approaches/138)找到关于Flickr架构的图片。大概描述如下：  
  -- 成对的ServerIron  
  ----Squid缓存  
  ------Net App's  
  ----PHP App服务器  
  ------存储管理器  
  ------主从切片  
  ------双树中央数据库  
  ------Memcached集群  
  ------大搜索引擎  
  -- 双树结构是一种MySQL的自定义修改设置，它能够通过增量式增加主服务器来实现扩展而不需要借助环的架构。这样能够廉价地进行扩展，因为相对于需要两倍硬件的主从式来说，它需要的硬件更少。  
  -- 中央数据库包含了像users表之类的数据。users表既包含了主用户键（少量的不同ID），也包含了指向用户数据所在切片的指针。  
* 使用专门的服务器处理静态内容。  
* 支持Unicode。  
* 使用“无共享”架构。  
* 除图片外的所有数据都存储在数据库中。  
* 无状态意味着能够调整服务器上的用户，这样使得API的穿件变得容易。  
* 在第一时间使用复制进行扩展，但这仅仅在读上有帮助。  
* 通过复制可能被搜索的部分数据库来创建搜索结果集。  
* 使用水平扩展，这样的话只需要简单地添加更多的机器就能满足新的需求。  
* 在PHP中解析每封邮件，找出并处理其中的图片。  
* 在早期，他们被主从的延迟所困扰。这里面有太大的负载，而且有但点问题。  
* 他们需要有在线维护、数据修复等等功能，而不需要关闭网站。  
* 关于容量规划有大量很好的材料。可以看看信息源中的材料来获取更多细节。  
* 为了便于将来的扩展，他们采用了以下方法:  
  -- 分片：当评论别人博客的时候，别人的数据存储在别人的分片上，但是别人对你评论的操作记录存在你的分片上。  
  -- 全局环：就像DNS一样，你需要知道去哪以及谁控制你去哪。每一次页面浏览都实时地计算出你的数据在哪。  
  -- PHP逻辑负责与切片的链接和保证数据的一致性（评论涉及10行代码）。  
* 切片：  
  -- 主数据库切片  
  -- 主动的主-主环复制：MySQL4.1上有些小缺点。自增ID自动保持主动性。  
  -- 根据新用户时产生的随机数来分配切片。  
  -- 随时进行的数据迁移能够让你删除某些重量级用户。当你有很多图片时，你需要做均衡。迁移192k的图片，700k的标签，大概需要3-4分钟。迁移工作是手动进行的。  
* 点击喜好  
  -- 从缓存中拖出图片所有者账号用来获取分片位置。  
  -- 从缓存中拖出我的信息，用来获取我的分片位置。  
  -- 开始一个分布式事务来回答下面问题：谁喜欢这张图片？我喜欢那些图片？  
* 能从任意分片问问题和恢复数据。这是很冗余的。  
* 为了防止复制延迟，做了以下事情：  
  -- 每次加载页面都将对应用户分配到一个桶里。  
  -- 如果服务器挂了，则连接到列表中的下一个服务器。如果所有的服务器都挂了，则显示粗物页面。不使用长连接，而是不停创建和关闭连接。每次加载页面都进行连接。  
* 每个用户的读写都在一个切片上。这样就不存在复制延迟了。  
* 分片上的每个服务器都是半负荷的。关闭了每个分片上一半的服务器。这样，当一个切片上的一台服务器挂了或者处于维护状态时，另一台服务器将能达到全负荷。升级的时候你只需要关闭并升级一半的切片，然后再处理另一半。  
* 他们打破了50%规则，长时间处于流量高峰期。QPS经常是6000-7000。但是设计是按照在50%的负载下最多4000次查询每秒。  
* 平均每个页面需要进行27-35次SQL查询。喜欢数是实时的。数据库访问API也都是实时的。要想实现实时性就要求不能有任何不足。  
* 如果不做流量控制的话会超过36k次查询每秒。特殊情况下会达到72k。  
* 每个切片处理40万以上的用户数据。
  -- 大量的数据存储了两份。比如，评论是评论者和被评论者关系的一部分。应该把评论存在哪呢？还是说两个地方都存着？事务被用来防止数据不同步。打开事务1，写入命令，打开事务2，写入命令，当一切正常的时候提交事务1，当事务1提交完毕后提交事务2。但是在事务1提交的时候如果服务出问题了那么还是存在失败的可能。  
* 搜索：  
  -- 两台搜索后端：分发35k qps到少量的切片和Yahoo!网页搜索。  
  -- 由于实时性的需求，所有者的单一标签搜索或者批量标签改变将会分发到切片上，剩下其他的都会分发到Yahoo!搜索中。  
  -- 你可以想象你有一个类似于Lucene搜索。  
* 硬件：  
  -- EMT64 w/RHEL4, 16GB RAM  
  -- 6-disk 15K RPM RAID-10  
  -- 数据大小为12TB的用户元信息（并不是图片，仅仅是innodb的idbata文件，图片文件更大）  
  -- 2U boxes。每个切片有大约120G的数据。  
* 备份流程：  
  -- 定时任务进行ibbackup，它能够在不同时间段内跨多切片运行。Hotbackup用作后备方案。  
  -- 每晚进行整个数据库集群的快照。  
  -- 一次写入或者删除多个巨大备份文件到复制文件存储系统中将会摧毁整个文件存储的性能，影响接下来几个小时的文件备份。在一个线上的图片存储上做这样的事情是件很坏的事。  
  -- 但是不管多贵，保留你所有数据多天的备份都是值得的。当你发现几天前的数据出错时你就会觉得保留多天的备份是多么有意义。你可以选择保留1,2,10或者30天。  
* 图片存储在文件中。当你上传时，系统会处理这些图片，求出其大小。这些元数据和文件指针都将存储到数据库中。  
* 聚合数据是很快的，因为它是在每个切片上进行的。  
* 设置每个切片的最大连接数为400，或者每个服务器和切片的最大连接数为800.保证充足的容量和连接。线程缓存设成了45，因为你不会有超过45个用户同时活动。  
* 标签：  
  -- 标签并不适合传统的规范的关系型数据库模式设计。反规范化或者大量的缓存是在微秒内为百万级标签生成标签云的唯一途径。  
  -- 一些数据视图是通过专用的处理集群进行离线计算并将结果保存到MySQL数据库中。因为有些关系是很难计算的，它会占用大量的数据库CPU时间。  
* 未来的方向
  -- 使用实时BCP来加速。这样所有的数据中心能够在同一时刻接收到对数据层（db, memcache等等）的写操作。一切都是实时的，没有任何事情会被挂起。  

##从中学到的
* **多思考，不要认为你的应用仅仅是Web应用**。你可以有REST API，SOAP API, RSS feeds, Atom feed等等。  
* **无状态化**。无状态化能让你的系统变得简单和更具鲁棒性。这样就可以毫无畏缩地处理升级问题。  
* 为你的傻冒数据库进行重构。  
* **容量规划**。 在产品讨论的早期进行容量规划。
* **开始的慢一点**。不要仅为了由你的站点增长引起的担忧或者高兴而买过多的设备。  
* **测量实际情况**。 容量规划只能基于真实发生的事情，而不是想象中的事情。  
* **构建日志记录和测量指标**。使用情况统计就像服务器统计一样重要。监理自定义的指标来衡量真实的使用情况和基于服务器的使用情况。  
* **缓存**。 缓存和内存是所有问题的答案。  
* **抽象**。 在数据库、商业逻辑、页面逻辑、页面标记和表示层之间创建一个清晰的抽象层。这样做能够支持快速迭代开发。  
* **分层**。 分层能够让开发者创建设计者用来构建用户体验的页面逻辑。当需要的时候设计者可以要求页面逻辑。这是两方的谈判。  
* **频繁发布**。 甚至每30分钟一次。  
* **忘记小的效率**。 过早优化是一切罪恶的根源。  
* **生产测试**。 建立到架构机制中，使得可以轻松地部署新硬件到生产中来。  
* **忘记基准**。 基准在获取容量大概信息时时管用的，但是在容量规划时不行。人造的测试只能给出人造的结果。真实环境下得到的结果才是更好的。  
* **寻找天花板**：  
  -- 每台服务器最多能干些什么事情？  
  -- 你离最大值有多远？趋势又是如何？  
  -- MySQL(磁盘IO?)  
  -- Squid（磁盘IO还是CPU?）  
  -- memcached(CPU还是网络？)
* **对你的应用类型的使用模式要敏感**  
  -- 你有事件相关增长吗？比如疾病，新闻事件。  
  -- Flickr在新年的第一个工作日的上传量相对于去年的峰值来说会上升20-40%。  
  -- 周日相对于其他天上传量平均多40-50%。  
* **对需求的指数增长保持敏感**。更多的用户意味着更多的内容，更多的内容意味着更多的连接，更多的连接意味着更多的使用。  
* **峰值预案**。能够处理峰值的上下行负载。














